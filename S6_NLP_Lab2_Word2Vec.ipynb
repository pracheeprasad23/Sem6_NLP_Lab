{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Prachee Prasad, 381060\n",
        "\n",
        "Lab 2: Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on data. Create embeddings using Word2Vec"
      ],
      "metadata": {
        "id": "N8o3Vnxl-LiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk gensim scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nBbuLCK_Q9b",
        "outputId": "8f5714dc-55cf-4801-db93-8cfe224be755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2k_S_ep_Ubo",
        "outputId": "9fcae125-e9e9-4a3d-d2d0-2b9bfc02bdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample dataset\n",
        "\n",
        "documents = [\n",
        "    \"NLP is a fascinating field of AI\",\n",
        "    \"AI and NLP are transforming the world\",\n",
        "    \"Machine learning is part of AI\",\n",
        "    \"We are learning NLP using Python\"\n",
        "]\n",
        "\n",
        "print(documents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhTk-QZd_W6T",
        "outputId": "a7de1496-3379-442b-d830-d25aff707cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP is a fascinating field of AI', 'AI and NLP are transforming the world', 'Machine learning is part of AI', 'We are learning NLP using Python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag-of-Words (Count Occurrence)\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_counts = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(count_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBag-of-Words (Count Occurrence):\")\n",
        "print(bow_counts.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_iUVQfY_am0",
        "outputId": "da16ed9b-16f9-433e-ff9b-8f98b7faf1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['ai' 'and' 'are' 'fascinating' 'field' 'is' 'learning' 'machine' 'nlp'\n",
            " 'of' 'part' 'python' 'the' 'transforming' 'using' 'we' 'world']\n",
            "\n",
            "Bag-of-Words (Count Occurrence):\n",
            "[[1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0]\n",
            " [1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1]\n",
            " [1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag-of-Words (Normalized Count Occurrence) (L2 Normalization)\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "count_vectorizer_norm = CountVectorizer()\n",
        "bow_counts = count_vectorizer_norm.fit_transform(documents)\n",
        "bow_normalized = normalize(bow_counts, norm='l2', axis=1)\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(count_vectorizer_norm.get_feature_names_out())\n",
        "\n",
        "print(\"\\nNormalized Bag-of-Words (L2 Normalization):\")\n",
        "print(bow_normalized.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXlQD38A_gpX",
        "outputId": "e22d5907-4faf-42bb-b11f-d910430200ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['ai' 'and' 'are' 'fascinating' 'field' 'is' 'learning' 'machine' 'nlp'\n",
            " 'of' 'part' 'python' 'the' 'transforming' 'using' 'we' 'world']\n",
            "\n",
            "Normalized Bag-of-Words (L2 Normalization):\n",
            "[[0.40824829 0.         0.         0.40824829 0.40824829 0.40824829\n",
            "  0.         0.         0.40824829 0.40824829 0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.37796447 0.37796447 0.37796447 0.         0.         0.\n",
            "  0.         0.         0.37796447 0.         0.         0.\n",
            "  0.37796447 0.37796447 0.         0.         0.37796447]\n",
            " [0.40824829 0.         0.         0.         0.         0.40824829\n",
            "  0.40824829 0.40824829 0.         0.40824829 0.40824829 0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.40824829 0.         0.         0.\n",
            "  0.40824829 0.         0.40824829 0.         0.         0.40824829\n",
            "  0.         0.         0.40824829 0.40824829 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Representation\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWfxaNRA_nt7",
        "outputId": "19d89f6a-cfeb-4314-9f48-718a4e08881e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['ai' 'and' 'are' 'fascinating' 'field' 'is' 'learning' 'machine' 'nlp'\n",
            " 'of' 'part' 'python' 'the' 'transforming' 'using' 'we' 'world']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.31685436 0.         0.         0.49641358 0.49641358 0.39137817\n",
            "  0.         0.         0.31685436 0.39137817 0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.27375357 0.42888787 0.33814012 0.         0.         0.\n",
            "  0.         0.         0.27375357 0.         0.         0.\n",
            "  0.42888787 0.42888787 0.         0.         0.42888787]\n",
            " [0.30880963 0.         0.         0.         0.         0.38144133\n",
            "  0.38144133 0.48380996 0.         0.38144133 0.48380996 0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.36559366 0.         0.         0.\n",
            "  0.36559366 0.         0.29597957 0.         0.         0.46370919\n",
            "  0.         0.         0.46370919 0.46370919 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Sentences for Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "print(tokenized_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvo8W13R_yvF",
        "outputId": "83540e81-7a2c-4a22-ac27-692a5107474d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['nlp', 'is', 'a', 'fascinating', 'field', 'of', 'ai'], ['ai', 'and', 'nlp', 'are', 'transforming', 'the', 'world'], ['machine', 'learning', 'is', 'part', 'of', 'ai'], ['we', 'are', 'learning', 'nlp', 'using', 'python']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec Model\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=tokenized_docs,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "a5Wpbd69_1Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embeddings (Vector Representation)\n",
        "print(\"Vector for word 'nlp':\")\n",
        "print(w2v_model.wv['nlp'])\n",
        "\n",
        "print(\"\\nVector size:\", w2v_model.wv.vector_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC1XaeoXABkN",
        "outputId": "4c5c890f-5991-47a7-c417-5f8b16989478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for word 'nlp':\n",
            "[-8.6207762e-03  3.6693334e-03  5.1942971e-03  5.7473937e-03\n",
            "  7.4650599e-03 -6.1747055e-03  1.1099416e-03  6.0544759e-03\n",
            " -2.8448051e-03 -6.1771143e-03 -4.0740482e-04 -8.3730584e-03\n",
            " -5.6021605e-03  7.1088444e-03  3.3524618e-03  7.2231065e-03\n",
            "  6.8022693e-03  7.5322362e-03 -3.7926226e-03 -5.6901621e-04\n",
            "  2.3529716e-03 -4.5188968e-03  8.3920360e-03 -9.8620411e-03\n",
            "  6.7665880e-03  2.9115782e-03 -4.9358848e-03  4.4020070e-03\n",
            " -1.7429485e-03  6.7098825e-03  9.9619338e-03 -4.3645748e-03\n",
            " -5.9599307e-04 -5.6999368e-03  3.8509031e-03  2.7887921e-03\n",
            "  6.8953354e-03  6.1001154e-03  9.5395697e-03  9.2723612e-03\n",
            "  7.8964084e-03 -6.9908407e-03 -9.1608996e-03 -3.5524677e-04\n",
            " -3.1000818e-03  7.8951921e-03  5.9356242e-03 -1.5428711e-03\n",
            "  1.5138414e-03  1.7952634e-03  7.8164274e-03 -9.5088966e-03\n",
            " -2.0412030e-04  3.4708700e-03 -9.3429489e-04  8.3816377e-03\n",
            "  9.0165604e-03  6.5354626e-03 -7.1288715e-04  7.7175130e-03\n",
            " -8.5373772e-03  3.2064954e-03 -4.6427255e-03 -5.0939051e-03\n",
            "  3.5908883e-03  5.3730384e-03  7.7714743e-03 -5.7624332e-03\n",
            "  7.4343458e-03  6.6272845e-03 -3.7086469e-03 -8.7436540e-03\n",
            "  5.4394249e-03  6.5085166e-03 -7.8311260e-04 -6.7079277e-03\n",
            " -7.0828749e-03 -2.4959052e-03  5.1439563e-03 -3.6696102e-03\n",
            " -9.3743131e-03  3.8316771e-03  4.8879450e-03 -6.4266520e-03\n",
            "  1.2078680e-03 -2.0728749e-03  2.7815922e-05 -9.8887291e-03\n",
            "  2.6948445e-03 -4.7458615e-03  1.0906652e-03 -1.5757994e-03\n",
            "  2.1983718e-03 -7.8840861e-03 -2.7113969e-03  2.6675414e-03\n",
            "  5.3426516e-03 -2.3951977e-03 -9.5107164e-03  4.5088506e-03]\n",
            "\n",
            "Vector size: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Similar Words using Word2Vec\n",
        "similar_words = w2v_model.wv.most_similar('ai')\n",
        "print(\"Words similar to 'ai':\")\n",
        "print(similar_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK87RaxXAFXq",
        "outputId": "17bab628-4a77-419b-eec7-7610b42d5611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'ai':\n",
            "[('fascinating', 0.21886461973190308), ('part', 0.21618622541427612), ('the', 0.09310683608055115), ('using', 0.09290151298046112), ('world', 0.07948420941829681), ('machine', 0.06284788995981216), ('field', 0.05455850437283516), ('we', 0.027049632743000984), ('python', 0.016147596761584282), ('nlp', -0.010376579128205776)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h--oHsfnAJu7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}