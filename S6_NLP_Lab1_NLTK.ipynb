{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1\n",
        "\n",
        "Prachee Prasad, 381060\n",
        "\n",
        "Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK\n",
        "library. Use porter stemmer and snowball stemmer for stemming. Use any technique for\n",
        "lemmatization."
      ],
      "metadata": {
        "id": "VmrwWMEgowtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeOCT19bo45i",
        "outputId": "79b618e9-f1c5-410d-cf85-8261b9d1073c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK Resources\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz3CSD2_pT_V",
        "outputId": "0b01c031-3815-4fbc-c1ef-a3baf516acfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Text\n",
        "text = \"NLTK is awesome! Let's learn NLP :)\"\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD8iTLw0pXw1",
        "outputId": "6f8d46e3-0005-401d-cddc-50b401d9f748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK is awesome! Let's learn NLP :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whitespace Tokenization\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "tokens_whitespace = wt.tokenize(text)\n",
        "print(\"Whitespace Tokenization:\", tokens_whitespace)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1xjojwepZZR",
        "outputId": "a6f07ed2-253e-4068-9382-01f60602cc5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization: ['NLTK', 'is', 'awesome!', \"Let's\", 'learn', 'NLP', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Punctuation-based Tokenization\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "pt = WordPunctTokenizer()\n",
        "tokens_punct = pt.tokenize(text)\n",
        "print(\"Punctuation-based Tokenization:\", tokens_punct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p_todPtpZ5t",
        "outputId": "acd5a25c-487b-493e-c579-cc3129ef9448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation-based Tokenization: ['NLTK', 'is', 'awesome', '!', 'Let', \"'\", 's', 'learn', 'NLP', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Treebank Tokenization\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tbt = TreebankWordTokenizer()\n",
        "tokens_treebank = tbt.tokenize(text)\n",
        "print(\"Treebank Tokenization:\", tokens_treebank)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETToI4nKpgly",
        "outputId": "e8533514-6670-41f7-ee70-78174428eadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokenization: ['NLTK', 'is', 'awesome', '!', 'Let', \"'s\", 'learn', 'NLP', ':', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tweet Tokenization\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "tokens_tweet = tt.tokenize(text)\n",
        "print(\"Tweet Tokenization:\", tokens_tweet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUNEBcnBpirg",
        "outputId": "600b3a14-8d62-4994-a042-c4eabdc23eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokenization: ['NLTK', 'is', 'awesome', '!', \"Let's\", 'learn', 'NLP', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Word Expression (MWE) Tokenization\n",
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('let', \"'s\"), ('openai',)])\n",
        "tokens_mwe = mwe.tokenize(text.lower().split())\n",
        "print(\"MWE Tokenization:\", tokens_mwe)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1l_0n7jpjMU",
        "outputId": "202f33da-b785-4ff5-b3fd-30f653c06c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MWE Tokenization: ['nltk', 'is', 'awesome!', \"let's\", 'learn', 'nlp', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "porter_stems = [ps.stem(word) for word in tokens_treebank]\n",
        "print(\"Porter Stemmer Output:\", porter_stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20wz9ZtgpkZl",
        "outputId": "434b3c8d-38fb-47a8-9f9a-2c6b3a514d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer Output: ['nltk', 'is', 'awesom', '!', 'let', \"'s\", 'learn', 'nlp', ':', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Snowball Stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "snowball_stems = [ss.stem(word) for word in tokens_treebank]\n",
        "print(\"Snowball Stemmer Output:\", snowball_stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHiQbbPbplvN",
        "outputId": "726a141f-35d9-4c05-9d00-b5f8cdb7b09d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball Stemmer Output: ['nltk', 'is', 'awesom', '!', 'let', \"'s\", 'learn', 'nlp', ':', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization (WordNet Lemmatizer)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_treebank]\n",
        "print(\"Lemmatization Output:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCcfX9_npm8t",
        "outputId": "970f4634-10ae-4e71-9f1b-3616bc4ad8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization Output: ['NLTK', 'is', 'awesome', '!', 'Let', \"'s\", 'learn', 'NLP', ':', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOnE7sQbqSD-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}